# 10 — Forge Baseline Findings

> **Status**: In progress
> **Created**: 2026-02-28
> **Session type**: Forge Guided Review — Baseline Session
> **Application under review**: Milk Jawn demo (`~/zivtech-demos/projects/milk-jawn/`)
> **Stakeholders**: Alex Urevick-Ackelsberg (CTO), GM/Owner (to be included in future sessions)
> **Context scenario**: Vibe-coded app — built iteratively with Claude, needs to be understood and made useful for the actual operator

---

## Context

The Milk Jawn demo is a vanilla JS staffing analytics and scheduling tool built across multiple Claude sessions. It's powered by real Square POS data (July 2022 - February 2026) and deployed on GitHub Pages. It has two main surfaces:

- **Dashboard** (index.html) — ~15 analytics widgets for business performance and staffing decisions
- **Shift Planner** (staffing-planner.html) — weekly scheduling with approvals, shift analysis, and settings

A subsequent effort to productize this demo into a multi-tenant platform (`joyus-ice-cream-shop`) produced infrastructure (APIs, multi-tenancy, admin panel, roles) but lost the operator intelligence that made the demo useful. The platform frontend doesn't call the scheduling engine endpoints for weekly metrics, trigger timing, weather impact, or overstaff assessment.

## Key Finding: The Demo Is Built for the Builder, Not the Operator

The CTO built the demo with Claude. The GM/owner — the actual daily user — finds it "extraordinarily confusing." The analytical models underneath are sound. The presentation fails to communicate them to the person who needs them.

This is not a feature problem. It's a communication problem. The app speaks the language of the data model, not the language of the operator.

## Stakeholder Clarification

- **Alex (CTO)**: Built the demo, understands the data model, can explain what widgets do but sometimes couldn't explain their controls (e.g., didn't know Monday %s were relative to Tuesday until this session)
- **GM/Owner**: The actual daily user. Finds the demo confusing. Would use the scheduling tool daily if it worked for her. The dashboard/analytics would be periodic (weekly/monthly). Has not yet participated in a Forge session directly.

## Findings by Category

### 1. Cross-Cutting Issues (Priority 1)

These affect every part of the application and must be addressed before individual features can be properly evaluated.

#### Language is system-centric, not operator-centric

| What the app says | What the operator thinks |
|-------------------|------------------------|
| "Wellbeing Load Signal" | "Are we burning people out?" |
| "Trigger Gap Planner" | "When do I need to hire for summer?" |
| "Scale Timing Monitor" | "Is it time to change our staffing level?" |
| "Only changes 7-day Monday-open projections and related scenario outputs" | "This changes how much we'd expect to make on Mondays" |
| "Low (55%) / Base (65%) / High (75%)" | ??? (nobody knows what these are percentages of) |

Every label, tooltip, and explanation was generated from the builder's mental model. The app needs to be rewritten in the operator's language.

#### Controls change numbers without explaining causality

Sliders and toggles affect displayed numbers, but the user can't trace what changed or why. Three questions must be answerable for every control:
1. What did I just change?
2. What numbers moved because of it?
3. Why do those numbers move that way?

If the operator can't answer all three, the control undermines trust in the entire system.

#### Numbers have no provenance

No number in the app traces back to its source. The CTO said he "would want to confirm them somehow." If the person who built it doesn't trust the numbers blindly, the operator certainly shouldn't be expected to. Every number needs:
- Where it came from (Square POS data, modeled estimate, industry benchmark, etc.)
- What date range it covers
- How it was calculated (at least at a summary level)
- Whether it's actual data or a projection

#### Help text compounded the confusion

An attempt was made to add help text to each widget. The help text was generated by Claude — the same system that built the confusing widgets. The explanations inherited the same assumptions and language that created the confusion in the first place. Help text made things worse, not better.

**Implication for Forge**: Explanations must be written from the operator's perspective, not the builder's. This is exactly why Forge's review process exists — to capture the "why" from the person who needs it, not the person who built it.

#### Data sourcing and attribution

The Industry Survey Lens shows benchmark data (peer medians, labor %, profit margins) but doesn't attribute where it comes from. If the data is fabricated or from a one-off survey, that's a trust problem.

**Action needed**: Identify real data sources for industry benchmarks:
- Paid providers (e.g., industry benchmark services)
- Free public data (BLS, trade association reports, USDA food service data)
- The operator's own historical data as self-benchmarking
- Clear labeling on every number: source, date, sample size

If real benchmark data can't be sourced, the feature should either be removed or clearly labeled as placeholder.

### 2. Dashboard Findings (Priority 2)

#### Information overload with no hierarchy

~15 widgets visible simultaneously. The operator doesn't know:
- Where to start — which widget answers her first question?
- What's important right now — are all 15 equally relevant this week?
- What to do about what she sees — "Labor % is 20.69%" means what? Is that good? Should she do something?

The dashboard treats everything as equally visible all the time. The operator's actual workflow is more like "tell me what needs my attention and let me drill in."

#### Widgets that are misplaced

**Observed Staff Pool** — shows which employees work at EP only, NL only, or cross-location. This is operational scheduling data, not business analytics. It belongs on the planner (when building a schedule: "who can I put at NL this week?"), not on the dashboard.

Other widgets may have similar placement issues. Each widget should be evaluated: does this belong on an analytics surface or an operational surface?

#### Monday scenario modeling is opaque

The Monday Sensitivity Panel lets users pick Low (55%) / Base (65%) / High (75%) scenarios. Problems:
- The percentages are relative to Tuesday's average revenue — this is never stated
- The range caps at 75% — why not 90% or 100%? What evidence sets the upper bound?
- Why Tuesday specifically? Is that the closest comparable day?
- The CTO didn't know what these numbers meant until the Forge baseline session

This widget has the right intent (help decide whether to open Mondays) but the execution is impenetrable.

#### Excel export has no clear job

The export button exists but nobody articulated what specific need it serves. Does the GM need to email a summary to the owner? Does the accountant need labor data for payroll? Without a clear job, the export is a feature without a reason.

### 3. Planner Findings (Priority 3)

#### Planning horizon is too short

The current planner thinks in single weeks. The business needs:
- **Months-ahead planning** — seasonal businesses need to plan staffing well in advance
- **Partial plans** — draft skeletons that get filled in and validated over time
- **Progressive refinement** — a plan 3 months out is rough; 2 weeks out it's detailed; day-of it's confirmed

#### No year-over-year context on recommendations

When the system recommends a staffing level, the GM/owner/manager need to understand it in context:
- "Last year in this season, this store averaged X people totaling Y hours"
- "We're now projecting A people and B hours"
- "Here's why the change is recommended"

When the system recommends a significant change, that's something the GM, owner, AND manager all need to understand. The comparison should be:
- Always available on every scheduled day
- Visually prominent only when there's a meaningful delta from historical pattern

#### Analyst notes are training data, not journals

The shift analysis "analyst notes" are currently treated as free-text journaling. They should be structured inputs for future automated planning:

**Recurring events** that affect scheduling:
- East Passyunk Hot Chocolate Crawl (EP only)
- Ice Cream for Breakfast Day (both locations)
- Other local events that predictably change demand

**Holiday calendars**:
- Closed: Thanksgiving, Christmas
- Early close: Christmas Eve
- Other modified schedules

**Pattern annotations**: Any note that implies "do this again next time this happens" should feed forward into the planning system, not just sit in a text field.

## Cross-Cutting Review Results

Full audit captured in `docs/planning/11-language-and-communication-audit.md`. Summary:

**Four problem types identified across the entire application:**

1. **Jargon labels** (20+ instances) — Section titles use system language: "Wellbeing Load Signal", "Trigger Gap Planner", "Scale Timing Monitor", "Performance Intelligence", "Expected Shift Viability". Operator translations proposed for each.

2. **System-facing descriptions** (12+ instances) — Help text and descriptions explain how the system works rather than what the user needs. App subtitle is a spec document feature list. Widget descriptions contain formulas ("Revenue × 72%") instead of plain-language explanations.

3. **Controls without causality** (4 major instances) — Monday demand percentages are relative to Tuesday (never stated, CTO didn't know). Shared manager time split doesn't explain what changes. Compare modes in Shift Analysis are incomprehensible. When controls change numbers, nothing visual shows what moved.

4. **Hidden parameters** (7 instances) — 72% margin used everywhere with no source. $28/hr manager rate hardcoded. Tuesday as Monday baseline never justified. "Modeled" months not explained. 55-75% Monday range has no stated evidence.

**The provenance principle** (proposed): every number should answer three questions — where does it come from, how current is it, can I change it?

## Review Queue (Proposed)

Based on this baseline session, the guided review should proceed:

1. **Cross-cutting issues** — language audit, provenance model, control causality — **COMPLETED (see doc 11)**
2. **Dashboard widgets** — evaluate each against "what decision does this help the operator make?" — **COMPLETED (see doc 12)**
3. **Planner features** — months-ahead planning, YoY context, event calendars, note-to-planning pipeline — **COMPLETED (see doc 13)**

## Open Items for Future Sessions

- [ ] Session with GM/Owner directly — validate these findings and language translations from her perspective
- [ ] Dashboard widget-by-widget review — which are useful, misplaced, or unnecessary?
- [ ] Planner workflow mapping — what does the GM's actual weekly/monthly scheduling process look like?
- [ ] Data source investigation for industry benchmarks
- [ ] Event calendar requirements — full inventory of recurring events and holidays
- [ ] Define what "partial plan" means operationally — what's in a skeleton vs. what gets filled in?
- [ ] Resolve the 72% margin assumption — is it measured, estimated, or configurable?
- [ ] Design visual causality for controls — how should changed numbers be highlighted?
- [ ] Strategic question: are "Lore" workflows (knowledge elicitation, guided review) and "Forge" workflows (observe current work, spec out middleware/replacements) subtasks of the same tool, or different tools? (See Forge open questions.)

---

*Forge Baseline Session conducted: February 28, 2026*
*Stakeholder: Alex Urevick-Ackelsberg (CTO)*
*Next session: Include GM/Owner for validation and additional perspective*
